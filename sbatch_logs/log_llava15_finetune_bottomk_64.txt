assigned gpus=2,3
[2024-12-05 03:14:54,442] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-12-05 03:14:57,962] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2024-12-05 03:14:57,963] [INFO] [runner.py:571:main] cmd = /home/airshad/miniforge3/envs/llava15_env/bin/python3.8 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 --deepspeed ./scripts/zero2.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version v1 --data_path /storage2/TEV/airshad/llava_finetuning/LLaVA/playground/data/LLaVA-Instruct-150K/llava_v1_5_mix665k.json --image_folder /storage2/TEV/airshad/llava_finetuning/LLaVA/playground/data/LLaVA-Instruct-150K/images --training_task finetuning --sampling_method bottomk --indexes_json_path /storage2/TEV/airshad/Sampling/data/INDEXES/FINETUNE/finetune_bottomk.json --num_visual_tokens 64 --vision_tower openai/clip-vit-large-patch14-336 --pretrain_mm_mlp_adapter /storage2/TEV/airshad/llava_finetuning/LLaVA/checkpoints/llava-v1.5-7b-bottomk-64-pretrain/mm_projector.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /storage2/TEV/airshad/llava_finetuning/LLaVA/checkpoints/llava-v1.5-7b-bottomk-64-lora --num_train_epochs 1 --per_device_train_batch_size 8 --per_device_eval_batch_size 4 --gradient_accumulation_steps 4 --evaluation_strategy no --save_strategy steps --save_steps 500 --save_total_limit 1 --learning_rate 2e-4 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to tensorboard
[2024-12-05 03:15:00,280] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-12-05 03:15:03,162] [INFO] [launch.py:138:main] 0 NCCL_P2P_DISABLE=1
[2024-12-05 03:15:03,162] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [2, 3]}
[2024-12-05 03:15:03,162] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2024-12-05 03:15:03,162] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2024-12-05 03:15:03,162] [INFO] [launch.py:163:main] dist_world_size=2
[2024-12-05 03:15:03,163] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=2,3
[2024-12-05 03:15:06,964] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-12-05 03:15:06,975] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-12-05 03:15:12,119] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-12-05 03:15:12,119] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-12-05 03:15:12,119] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-12-05 03:15:13,195] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 4099345
[2024-12-05 03:15:13,196] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 4099346
[2024-12-05 03:15:13,266] [ERROR] [launch.py:321:sigkill_handler] ['/home/airshad/miniforge3/envs/llava15_env/bin/python3.8', '-u', 'llava/train/train_mem.py', '--local_rank=1', '--lora_enable', 'True', '--lora_r', '128', '--lora_alpha', '256', '--mm_projector_lr', '2e-5', '--deepspeed', './scripts/zero2.json', '--model_name_or_path', 'lmsys/vicuna-7b-v1.5', '--version', 'v1', '--data_path', '/storage2/TEV/airshad/llava_finetuning/LLaVA/playground/data/LLaVA-Instruct-150K/llava_v1_5_mix665k.json', '--image_folder', '/storage2/TEV/airshad/llava_finetuning/LLaVA/playground/data/LLaVA-Instruct-150K/images', '--training_task', 'finetuning', '--sampling_method', 'bottomk', '--indexes_json_path', '/storage2/TEV/airshad/Sampling/data/INDEXES/FINETUNE/finetune_bottomk.json', '--num_visual_tokens', '64', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--pretrain_mm_mlp_adapter', '/storage2/TEV/airshad/llava_finetuning/LLaVA/checkpoints/llava-v1.5-7b-bottomk-64-pretrain/mm_projector.bin', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/storage2/TEV/airshad/llava_finetuning/LLaVA/checkpoints/llava-v1.5-7b-bottomk-64-lora', '--num_train_epochs', '1', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '4', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-4', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'tensorboard'] exits with return code = 1
